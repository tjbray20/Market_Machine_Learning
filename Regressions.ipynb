{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Regresssions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents:\n",
    "- [Library Imports](#Library-Imports)\n",
    "- [Preprocessing the Data](#Preprocessing-the-Data)\n",
    "- [Models](#Models)\n",
    "    - [Elastic Net](#Elastic-Net)\n",
    "    - [Support Vector Regression](#Support-Vector-Regression)\n",
    "    - [Ridge](#Ridge)\n",
    "- [Looping Through the Data](#Looping-Through-the-Data)\n",
    "- [Residual Plots](#Residual-Plots)\n",
    "- [Results](#Results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import LinearSVR\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes highly correlated variables within the dataset. The function looks at the \n",
    "# Pearson Correlation Coefficient between every pair of variables in the dataset and returns all \n",
    "# of those between .8 and .999. While working with data from different companies, I found that by\n",
    "# removing the first 90% of correlated variables within the DataFrame, in almost all cases, there\n",
    "# were no longer correlations within the dataset.\n",
    "\n",
    "def remove_corrs(df):\n",
    "    \n",
    "    # create a DataFrame with all the of the correlation coefficients, and then stack the pairings.\n",
    "    corrs = df.corr().stack().reset_index()\n",
    "    corrs.columns = ['1','2','R2']\n",
    "    # create a temoporary DataFrame that contains all the pairs of variables that have a correlation\n",
    "    # between .8 and .999. Because the .corr() method returns the pairings in both directions, I only\n",
    "    # look at half of the cells because of duplicates.\n",
    "    temp = corrs[(corrs.R2 > .8) & (corrs.R2 < .999)].sort_values('R2', ascending = False).reset_index(drop = True)\n",
    "    correlations = temp[temp.index % 2 == 0]\n",
    "    \n",
    "    # look at how many times each variable shows up in the correlation column\n",
    "    corr_index = correlations['1'].value_counts().index\n",
    "    # delete the first 90% of the variables. This generally removes the columns that are most correlated with the others. \n",
    "    to_drop = round(len(corr_index)*.9)\n",
    "    \n",
    "    df2 = df.drop(corr_index[:to_drop], axis = 1)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preprocesses my data so it is ready for distanced based regressions.\n",
    "\n",
    "def preprocess_data(df, target_var = 'close'):\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Here, I shift all the independent variables back one day in order to make sure there is\n",
    "    # no data leakage. Because we're trying to predict closing price, we can't know many of\n",
    "    # things that happen throughout the day we're trying to predict.\n",
    "    X = df.shift(1).dropna()\n",
    "    \n",
    "    # This is the function described above.\n",
    "    X_data = remove_corrs(X)\n",
    "\n",
    "    # Because I am running a regression, I can use a continuous variable as my dependent variable.\n",
    "    # Ideally, my algorithim can predict the closing price at the end of the day with accuracy.\n",
    "    y = df[target_var].iloc[1:]\n",
    "    \n",
    "    # Do a train test split with the first 80% of the data being the training set and the last 20%\n",
    "    # as the testing set.\n",
    "    train_num = round(len(X)*.8)\n",
    "    test_num = round(len(X)*.2)\n",
    "    print(train_num, test_num)\n",
    "\n",
    "    X_train = X_data.iloc[:train_num]\n",
    "    X_test = X_data.iloc[-test_num:]\n",
    "    y_train = y.iloc[:train_num]\n",
    "    y_test = y.iloc[-test_num:]\n",
    "    \n",
    "    # Because I am doing distance based regressions, I need to scale the data so that variables with\n",
    "    # higher absolute values don't dominate the metrics.\n",
    "    ss = StandardScaler()\n",
    "    X_train_scaled = ss.fit_transform(X_train)\n",
    "    X_test_scaled = ss.transform(X_test)\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns).set_index(X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns).set_index(X_test.index)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the process, I tried many different Regression models, the three that follow, along with Linear OLS and Rolling OLS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EN_test(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'l1_ratio': [.01,.25,.5,.75,1],\n",
    "    'alpha': [.01,.25,.5,.75,1],\n",
    "    }\n",
    "    \n",
    "    # Instantiate & fit RidgeRegression\n",
    "    en = ElasticNet(random_state = 42, max_iter = 10e5)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=en, param_grid=grid, cv=cv, scoring='neg_root_mean_squared_error', n_jobs = -1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_hat_train = gs.predict(X_train)\n",
    "    y_hat_test = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv, scoring = 'neg_root_mean_squared_error').mean()\n",
    "\n",
    "    # Score the train and test sets.\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    \n",
    "    print(f'Train RMSE: {train_score}')\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    print(f'Test RMSE: {test_score}')\n",
    "\n",
    "    # Create a DataFrame that has the predictions for the given days of the train set.\n",
    "    preds_train = pd.concat([y_train, pd.DataFrame(y_hat_train, columns = ['predictions'], index = y_train.index), df.day_direction], axis = 1)\n",
    "    preds_train.dropna(inplace = True)\n",
    "\n",
    "    # Compare the direction of the predicted value with what actually happened on the given day.\n",
    "    preds_train['direction'] = np.where(preds_train.predictions > preds_train.adjustedclose.shift(1), 1, 0)\n",
    "    preds_train['correct'] = np.where(preds_train['direction'] == preds_train['day_direction'], 1, 0)\n",
    "    train_accuracy = preds_train['correct'].value_counts(normalize = True)\n",
    "    \n",
    "    # Create a DataFrame that has the predictions for the given days of the test set.\n",
    "    preds_test = pd.concat([y_test, pd.DataFrame(y_hat_test, columns = ['predictions'], index = y_test.index), df.day_direction], axis = 1)\n",
    "    preds_test.dropna(inplace = True)\n",
    "\n",
    "    # Compare the direction of the predicted value with what actually happend on the given day. \n",
    "    preds_test['direction'] = np.where(preds_test.predictions > preds_test.adjustedclose.shift(1), 1, 0)\n",
    "    preds_test['correct'] = np.where(preds_test['direction'] == preds_test['day_direction'], 1, 0)\n",
    "    test_accuracy = preds_test['correct'].value_counts(normalize = True)\n",
    "\n",
    "    print(f'Train accuracy: {train_accuracy[1]}')\n",
    "    print(f'Test accuracy: {test_accuracy[1]}')\n",
    "    \n",
    "    normality_score = normaltest(y_hat_train - y_train)[1]\n",
    "    print(f'Normality score: {normality_score}')\n",
    "\n",
    "    results = ['Elastic Net', train_score, cv_results, test_score, train_accuracy[1], test_accuracy[1], normality_score]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVR_test(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    svr = LinearSVR(random_state = 42, max_iter = 10e5)\n",
    "    svrmodel = svr.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = svr.score(X_train, y_train)\n",
    "    y_hat_train = svr.predict(X_train)\n",
    "    train_rmse = mean_squared_error(y_train, y_hat_train, squared = False)\n",
    "    train_cv_score = cross_val_score(svrmodel, X_train, y_train, scoring = 'neg_root_mean_squared_error').mean()\n",
    "    \n",
    "    test_score = svr.score(X_test, y_test)\n",
    "    y_hat_test = svr.predict(X_test)\n",
    "    test_rmse = mean_squared_error(y_test, y_hat_test, squared = False)\n",
    "    \n",
    "    preds_train = pd.concat([y_train, pd.DataFrame(y_hat_train, columns = ['predictions'], index = y_train.index), df.day_direction], axis = 1)\n",
    "    preds_train.dropna(inplace = True)\n",
    "\n",
    "    preds_train['direction'] = np.where(preds_train.predictions > preds_train.adjustedclose.shift(1), 1, 0)\n",
    "    preds_train['correct'] = np.where(preds_train['direction'] == preds_train['day_direction'], 1, 0)\n",
    "    train_accuracy = preds_train['correct'].value_counts(normalize = True)\n",
    "    \n",
    "    preds_test = pd.concat([y_test, pd.DataFrame(y_hat_test, columns = ['predictions'], index = y_test.index), df.day_direction], axis = 1)\n",
    "    preds_test.dropna(inplace = True)\n",
    "\n",
    "    preds_test['direction'] = np.where(preds_test.predictions > preds_test.adjustedclose.shift(1), 1, 0)\n",
    "    preds_test['correct'] = np.where(preds_test['direction'] == preds_test['day_direction'], 1, 0)\n",
    "    test_accuracy = preds_test['correct'].value_counts(normalize = True)\n",
    "    \n",
    "    print(f'Train RMSE : {train_rmse}')\n",
    "    print(f'Mean Train Cross-Validation RMSE: {train_cv_score}')\n",
    "    print(f'Test RMSE : {test_rmse}')\n",
    "    print(f'Train accuracy: {train_accuracy[1]}')\n",
    "    print(f'Test accuracy: {test_accuracy[1]}')\n",
    "\n",
    "    normality_score = normaltest(y_hat_train - y_train)[1]\n",
    "    print(f'Normality score: {normality_score}')\n",
    "\n",
    "    results = ['SVR', train_score, train_cv_score, test_score, train_accuracy[1], test_accuracy[1], normality_score]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_test(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "    'alpha': [.01, .05, .1,.5,.75,1],\n",
    "    }\n",
    "    \n",
    "    # Instantiate & fit RidgeRegression\n",
    "    ridge = Ridge(random_state = 42, max_iter = 10e5)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=ridge, param_grid=grid, cv=cv, scoring='r2', n_jobs = -1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    ridge_params = gs.best_params_\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_hat_train = gs.predict(X_train)\n",
    "    y_hat_test = gs.predict(X_test)\n",
    "\n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    train_cv_score = cross_val_score(gs, X_train, y_train, cv=cv, scoring = 'neg_root_mean_squared_error').mean()\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "\n",
    "    train_rmse = mean_squared_error(y_train, y_hat_train, squared = False)\n",
    "    test_rmse = mean_squared_error(y_test, y_hat_test, squared = False)\n",
    "    \n",
    "    preds_train = pd.concat([y_train, pd.DataFrame(y_hat_train, columns = ['predictions'], index = y_train.index), df.day_direction], axis = 1)\n",
    "    preds_train.dropna(inplace = True)\n",
    "\n",
    "    preds_train['direction'] = np.where(preds_train.predictions > preds_train.adjustedclose.shift(1), 1, 0)\n",
    "    preds_train['correct'] = np.where(preds_train['direction'] == preds_train['day_direction'], 1, 0)\n",
    "    train_accuracy = preds_train['correct'].value_counts(normalize = True)\n",
    "    \n",
    "    preds_test = pd.concat([y_test, pd.DataFrame(y_hat_test, columns = ['predictions'], index = y_test.index), df.day_direction], axis = 1)\n",
    "    preds_test.dropna(inplace = True)\n",
    "\n",
    "    preds_test['direction'] = np.where(preds_test.predictions > preds_test.adjustedclose.shift(1), 1, 0)\n",
    "    preds_test['correct'] = np.where(preds_test['direction'] == preds_test['day_direction'], 1, 0)\n",
    "    test_accuracy = preds_test['correct'].value_counts(normalize = True)\n",
    "    \n",
    "    print(f'Train RMSE : {train_rmse}')\n",
    "    print(f'Mean Train Cross-Validation RMSE: {train_cv_score}')\n",
    "    print(f'Test RMSE : {test_rmse}')\n",
    "    print(f'Train accuracy: {train_accuracy[1]}')\n",
    "    print(f'Test accuracy: {test_accuracy[1]}')\n",
    " \n",
    "    normality_score = normaltest(y_hat_train - y_train)[1]\n",
    "    print(f'Normality score: {normality_score}')\n",
    "\n",
    "    results = ['Ridge', train_rmse, train_cv_score, test_rmse, train_accuracy[1], test_accuracy[1], normality_score]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping Through the Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the files in the Formatted Data folder.\n",
    "files = [f for f in listdir('.\\FormattedData')]\n",
    "\n",
    "# create a list of companies that can be analyzed.\n",
    "companies = []\n",
    "for i in files:\n",
    "    company = i.split('.')[0]\n",
    "    companies.append(company)\n",
    "\n",
    "# create a list of random companies to analyze from within the formatted companies\n",
    "test_companies = np.random.choice(companies, 4, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the companies and perform the different alogrithims. Create a\n",
    "# list of DataFrames that can then be used to compared results across different\n",
    "# companies and algorithms.\n",
    "\n",
    "#results = []\n",
    "\n",
    "#for c in test_companies:\n",
    "#    csv_for_df = f'FormattedData/{c}.csv'\n",
    "#    df = pd.read_csv(csv_for_df, index_col = 0)\n",
    "#    X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "#    en_results = EN_test(X_train, X_test, y_train, y_test)\n",
    "#    svr_results = SVR_test(X_train, X_test, y_train, y_test)\n",
    "#    ridge_results = Ridge_test(X_train, X_test, y_train, y_test)\n",
    "#    c_results = pd.DataFrame([en_results, svr_results, ridge_results],\n",
    "#            columns = ['Model Type', 'Train RMSE', 'Cross-Val RMSE', 'Test RMSE', \n",
    "#             'Train Accuracy', 'Test Accuracy', 'Normality of Residuals'])\n",
    "#    c_results['company'] = c.split('_')[0]\n",
    "#    results.append(c_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results\n",
    "#tests = pd.concat(results)\n",
    "#tests.to_csv('normregressionresults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b02a18ba7f50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'normregressionresults.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.read_csv('normregressionresults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (24,7))\n",
    "plot_x = y_train.index\n",
    "plot_y = (y_hat_train - y_train)/y_train\n",
    "plt.scatter(plot_x,plot_y)\n",
    "ax.yaxis.set_major_formatter('{x:1,.2%}')\n",
    "plt.axhline(y=0, alpha = .5, color = 'red', linewidth = 4.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots(figsize = (24,7))\n",
    "ax.scatter(y_test.index, (y_hat_test - y_test)/y_test)\n",
    "ax.yaxis.set_major_formatter('{x:1,.2%}')\n",
    "plt.axhline(y=0, alpha = 1, color = 'red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax = plt.subplots(figsize = (24,7))\n",
    "fig3_data = preds_test[(preds_test.index >= '1-1-2020') & (preds_test.index <= '4-1-2021')]\n",
    "ax.plot(fig3_data.adjustedclose, linewidth = 3.0)\n",
    "ax.plot(fig3_data.predictions)\n",
    "ax.yaxis.set_major_formatter('${x:1,.2f}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax = plt.subplots(figsize = (24,7))\n",
    "fig4_data = preds_train[(preds_train.index >= '1-1-2018') & (preds_train.index <= '1-1-2020')]\n",
    "ax.plot(fig4_data.adjustedclose, linewidth = 3.0)\n",
    "ax.plot(fig4_data.predictions)\n",
    "ax.yaxis.set_major_formatter('${x:1,.2f}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
