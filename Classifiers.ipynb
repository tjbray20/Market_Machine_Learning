{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classifiers\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents:\n",
    "- [Library Imports](#Library-Imports)\n",
    "- [Preprocessing the Data](#Preprocessing-the-Data)\n",
    "- [Models](#Models)\n",
    "    - [Logistic Regression](#Logistic-Regression)\n",
    "    - [Decision Tree](#Decision-Tree)\n",
    "    - [XGBoost](#XGBoost)\n",
    "- [Looping Through the Data](#Looping-Through-the-Data)\n",
    "- [Results](#Results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes highly correlated variables within the dataset. The function looks at the \n",
    "# Pearson Correlation Coefficient between every pair of variables in the dataset and returns all \n",
    "# of those between .8 and .999. While working with data from different companies, I found that by\n",
    "# removing the first 90% of correlated variables within the DataFrame, in almost all cases, there\n",
    "# were no longer correlations within the dataset.\n",
    "\n",
    "def remove_corrs(df):\n",
    "    \n",
    "    corrs = df.corr().stack().reset_index()\n",
    "    corrs.columns = ['1','2','R2']\n",
    "    temp = corrs[(corrs.R2 > .8) & (corrs.R2 < .999)].sort_values('R2', ascending = False).reset_index(drop = True)\n",
    "    correlations = temp[temp.index % 2 == 0]\n",
    "    \n",
    "    corr_index = correlations['1'].value_counts().index\n",
    "    to_drop = round(len(corr_index)*.9)\n",
    "    \n",
    "    df2 = df.drop(corr_index[:to_drop], axis = 1)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preprocesses my data so it is ready for distanced based classifiers.\n",
    "\n",
    "def preprocess_data(df, target_var = 'close'):\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Here, I shift all the independent variables back one day in order to make sure there is\n",
    "    # no data leakage. Because we're trying to predict closing price, we can't know many of\n",
    "    # things that happen throughout the day we're trying to predict.\n",
    "    X = df.shift(1).dropna()\n",
    "    \n",
    "    # This is the function described above.\n",
    "    X_data = remove_corrs(X)\n",
    "\n",
    "    # Because I am running a regression, I can use a continuous variable as my dependent variable.\n",
    "    # Ideally, my algorithim can predict the closing price at the end of the day with accuracy.\n",
    "    y = df[target_var].iloc[1:]\n",
    "    \n",
    "    # Do a train test split with the first 80% of the data being the training set and the last 20%\n",
    "    # as the testing set.\n",
    "    train_num = round(len(X)*.8)\n",
    "    test_num = round(len(X)*.2)\n",
    "    print(train_num, test_num)\n",
    "\n",
    "    X_train = X_data.iloc[:train_num]\n",
    "    X_test = X_data.iloc[-test_num:]\n",
    "    y_train = y.iloc[:train_num]\n",
    "    y_test = y.iloc[-test_num:]\n",
    "    \n",
    "    # Because I am doing distance based regressions, I need to scale the data so that variables with\n",
    "    # higher absolute values don't dominate the metrics.\n",
    "    ss = StandardScaler()\n",
    "    X_train_scaled = ss.fit_transform(X_train)\n",
    "    X_test_scaled = ss.transform(X_test)\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns).set_index(X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns).set_index(X_test.index)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'penalty': ['l1', 'l2' ,'elasticnet'],\n",
    "    'solver': ['newton-cg','lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "    \n",
    "    # Instantiate & fit LogReg model for GridSearch\n",
    "    grid_logreg = LogisticRegression(random_state=42)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_logreg, param_grid=grid, cv=cv,\n",
    "                      scoring='accuracy', n_jobs = -1, error_score = 0)\n",
    "    gs.fit(X_train, y_train)\n",
    "        \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv).mean()\n",
    "\n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "\n",
    "    print(f'Train Mean Accuracy: {train_score}')\n",
    "    print(f'Mean Cross-Val Accuracy: {cv_results}')\n",
    "    print(f'Test Mean Accuracy: {test_score}')\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test)\n",
    "    \n",
    "    results = ['random forest', train_score, cv_results, test_score]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtree(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'max_depth': [3,10,15],\n",
    "    'min_samples_split': [2,8,10,15],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    # Instantiate & fit Decision Tree model for GridSearch\n",
    "    grid_dt = DecisionTreeClassifier()\n",
    "    grid_dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_dt, param_grid=grid, cv=cv, scoring='accuracy', n_jobs = -1)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv).mean()\n",
    "\n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    \n",
    "    print(f'Train Mean Accuracy: {train_score}')\n",
    "    print(f'Mean Cross-Val Accuracy: {cv_results}')\n",
    "    print(f'Test Mean Accuracy: {test_score}')\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test)\n",
    "    \n",
    "    results = ['decision tree', train_score, cv_results, test_score]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'learning_rate': [.01,.05,.1,.5,1],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [3],\n",
    "    'subsample': [1],\n",
    "    'n_estimators': [100,500]}\n",
    "    \n",
    "    # Instantiate & fit XGClassifier\n",
    "    xgb = XGBClassifier(verbosity=0, random_state=42)\n",
    "    #xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=xgb, param_grid=grid, cv=cv, scoring='accuracy', n_jobs = -1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv).mean()\n",
    "\n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "\n",
    "    print(f'Train Mean Accuracy: {train_score}')\n",
    "    print(f'Mean Cross-Val Score: {cv_results}')\n",
    "    print(f'Test Mean Accuracy: {test_score}')\n",
    "\n",
    "    \n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test);\n",
    "    \n",
    "    results = ['xgboost', train_score, cv_results, test_score]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping Through the Data\n",
    "***\n",
    "This is the code that was used to loop through the data and get the final csv files with the regression results. Due to time and processing power constraints, the code was run using [Google Colab](#https://colab.research.google.com/?utm_source=scs-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the files in the Formatted Data folder.\n",
    "files = [f for f in listdir('.\\FormattedData')]\n",
    "\n",
    "#remove the ipynb checkpoint file\n",
    "files.pop(0)\n",
    "\n",
    "# create a list of companies that can be analyzed.\n",
    "companies = []\n",
    "for i in files:\n",
    "    company = i.split('.')[0]\n",
    "    companies.append(company)\n",
    "\n",
    "# create a list of random companies to analyze from within the formatted companies\n",
    "test_companies = np.random.choice(companies, 5, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_companies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d68bf031836b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_companies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mcsv_for_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'FormattedData/{c}.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_for_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_companies' is not defined"
     ]
    }
   ],
   "source": [
    "# loop through the companies and perform the different alogrithims. Create a\n",
    "# list of DataFrames that can then be used to compared results across different\n",
    "# companies and algorithms.\n",
    "\n",
    "results = []\n",
    "\n",
    "for c in test_companies:\n",
    "    csv_for_df = f'FormattedData/{c}.csv'\n",
    "    df = pd.read_csv(csv_for_df, index_col = 0)\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "    logreg_results = logreg(X_train, X_test, y_train, y_test)\n",
    "    dtree_results = dtree(X_train, X_test, y_train, y_test)\n",
    "    randomforest_results = random_forest(X_train, X_test, y_train, y_test)\n",
    "    xgboost_results = xgboost(X_train, X_test, y_train, y_test)\n",
    "    c_results = pd.DataFrame([logreg_results, dtree_results, randomforest_results, xgboost_results],\n",
    "            columns = ['Model Type', 'Train Accuracy', 'Cross Val Accuracy','Test Accuracy'])\n",
    "    c_results['company'] = c.split('_')[0]\n",
    "    results.append(c_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
